{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "752130c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # If running in a notebook, install missing dependencies with pip before import\n",
    "# import sys\n",
    "# import subprocess\n",
    "\n",
    "# def install_and_import(package, import_name=None):\n",
    "#     import importlib\n",
    "#     try:\n",
    "#         if import_name is None:\n",
    "#             import_name = package\n",
    "#         importlib.import_module(import_name)\n",
    "#     except ImportError:\n",
    "#         print(f\"Installing {package} ...\")\n",
    "#         subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "#         # Optionally try to import again\n",
    "#         importlib.invalidate_caches()\n",
    "#         importlib.import_module(import_name)\n",
    "\n",
    "# # List of (pip_package, import_name) pairs\n",
    "# packages = [\n",
    "#     (\"torch\", \"torch\"),\n",
    "#     (\"torchvision\", \"torchvision\"),\n",
    "#     (\"datasets\", \"datasets\"),\n",
    "#     (\"numpy\", \"numpy\"),\n",
    "# ]\n",
    "\n",
    "# for pip_name, import_name in packages:\n",
    "#     install_and_import(pip_name, import_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c482f68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1073cbdf0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from ResNet import ResNet\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "import glob\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_built() \\\n",
    "    else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(device)\n",
    "\n",
    "torch.manual_seed(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccd12f1",
   "metadata": {},
   "source": [
    "## __Using pre-trained model - CIFAR100__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9eb1962e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/ronibendom/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    }
   ],
   "source": [
    "#Im using model weights from pytorch-cifar-models: https://github.com/chenyaofo/pytorch-cifar-models\n",
    "#Expected accuracy for ResNET56 on CIFAR100 is 72.63%\n",
    "model = torch.hub.load(\"chenyaofo/pytorch-cifar-models\", \"cifar100_resnet56\", pretrained=True).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75ac007",
   "metadata": {},
   "source": [
    "#### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d714fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define standard data transforms for CIFAR100\n",
    "# CIFAR100 mean and std:\n",
    "# mean = [0.5071, 0.4867, 0.4408], std = [0.2675, 0.2565, 0.2761]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.5071, 0.4867, 0.4408],\n",
    "        std=[0.2675, 0.2565, 0.2761]\n",
    "    ),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.5071, 0.4867, 0.4408],\n",
    "        std=[0.2675, 0.2565, 0.2761]\n",
    "    ),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a235e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_dataset=torchvision.datasets.CIFAR100(root='./data',train=True,download=True,transform=train_transform)\n",
    "test_dataset=torchvision.datasets.CIFAR100(root='./data',train=False,download=True,transform=test_transform)\n",
    "\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1bbdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verify accuracy of pre-trained model\n",
    "model.eval()\n",
    "correct_val = 0\n",
    "total_val = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        output = model(images)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        total_val += labels.size(0)\n",
    "        correct_val += (predicted == labels).sum().item()\n",
    "    val_acc = correct_val / total_val if total_val > 0 else 0.\n",
    "\n",
    "print(val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a03ca33",
   "metadata": {},
   "source": [
    "Given the results, extracting learning parameters from their log in https://cdn.jsdelivr.net/gh/chenyaofo/pytorch-cifar-models@logs/logs/cifar100/resnet56/default.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d3aaff",
   "metadata": {},
   "source": [
    "## __Initial training of ResNet-56 on CIFAR100__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cc0bae",
   "metadata": {},
   "source": [
    "#### NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0228fa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ResNet(num_classes=100,n=9).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0427d231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_dataset=torchvision.datasets.CIFAR100(root='./data',train=True,download=True,transform=train_transform)\n",
    "test_dataset=torchvision.datasets.CIFAR100(root='./data',train=False,download=True,transform=test_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89ddfd4",
   "metadata": {},
   "source": [
    "#### Define basic params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc28a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training hyperparameters\n",
    "num_epochs = 200\n",
    "batch_size = 256\n",
    "initial_lr = 0.1\n",
    "momentum = 0.9\n",
    "weight_decay = 5e-4\n",
    "eta_min = 0.0\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=initial_lr,\n",
    "    momentum=momentum,\n",
    "    weight_decay=weight_decay,\n",
    "    nesterov=True\n",
    ")\n",
    "\n",
    "# Scheduler\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=num_epochs,   # total epochs\n",
    "    eta_min=eta_min  # min LR\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bebd6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1952e2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint loading functionality\n",
    "def find_latest_checkpoint(checkpoint_dir='checkpoints'):\n",
    "    \"\"\"Find the latest checkpoint file in the directory\"\"\"\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        return None\n",
    "    \n",
    "    checkpoint_files = glob.glob(os.path.join(checkpoint_dir, 'resnet_epoch_*.pth'))\n",
    "    if not checkpoint_files:\n",
    "        return None\n",
    "    \n",
    "    # Sort by epoch number (extract epoch number from filename)\n",
    "    def extract_epoch_number(filename):\n",
    "        # Extract epoch number from filename like 'resnet_epoch_50.pth'\n",
    "        basename = os.path.basename(filename)\n",
    "        epoch_str = basename.split('_epoch_')[1].split('.pth')[0]\n",
    "        return int(epoch_str)\n",
    "    \n",
    "    latest_checkpoint = max(checkpoint_files, key=extract_epoch_number)\n",
    "    return latest_checkpoint\n",
    "\n",
    "def load_checkpoint(model, optimizer, scheduler, checkpoint_path):\n",
    "    \"\"\"Load checkpoint and return epoch number and metrics\"\"\"\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        print(f\"Checkpoint not found: {checkpoint_path}\")\n",
    "        return 0, [], [], []\n",
    "    \n",
    "    print(f\"Loading checkpoint from: {checkpoint_path}\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    # Load model state\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    # Load optimizer state\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    # Load scheduler state\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    \n",
    "    # Get epoch number\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    \n",
    "    print(f\"Resuming from epoch {start_epoch}\")\n",
    "    print(f\"Previous metrics - Loss: {checkpoint['train_loss']:.4f}, \"\n",
    "          f\"Train Acc: {checkpoint['train_acc']:.4f}, Val Acc: {checkpoint['val_acc']:.4f}\")\n",
    "    \n",
    "    return start_epoch, checkpoint.get('train_losses', []), \\\n",
    "           checkpoint.get('train_accuracies', []), checkpoint.get('val_accuracies', [])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2f77fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced training loop with checkpoint resuming\n",
    "def train_with_resume(model, optimizer, scheduler, criterion, train_loader, test_loader, \n",
    "                     num_epochs, device, resume_from_checkpoint=True):\n",
    "    \"\"\"\n",
    "    Training loop that can resume from the latest checkpoint\n",
    "    \"\"\"\n",
    "    # Initialize metrics lists\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    # Create directory for saving weights\n",
    "    os.makedirs('checkpoints', exist_ok=True)\n",
    "    \n",
    "    # Try to resume from checkpoint if requested\n",
    "    start_epoch = 0\n",
    "    if resume_from_checkpoint:\n",
    "        latest_checkpoint = find_latest_checkpoint()\n",
    "        if latest_checkpoint:\n",
    "            start_epoch, train_losses, train_accuracies, val_accuracies = load_checkpoint(\n",
    "                model, optimizer, scheduler, latest_checkpoint\n",
    "            )\n",
    "            print(f\"Resuming training from epoch {start_epoch + 1}\")\n",
    "        else:\n",
    "            print(\"No checkpoint found, starting from epoch 1\")\n",
    "    else:\n",
    "        print(\"Starting fresh training from epoch 1\")\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(images)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        train_acc = correct / total if total > 0 else 0.\n",
    "\n",
    "        del images, labels, output, loss\n",
    "\n",
    "        model.eval()\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                output = model(images)\n",
    "                _, predicted = torch.max(output, 1)\n",
    "                total_val += labels.size(0)\n",
    "                correct_val += (predicted == labels).sum().item()\n",
    "            val_acc = correct_val / total_val if total_val > 0 else 0.\n",
    "        \n",
    "        del images, labels, output\n",
    "\n",
    "        train_losses.append(avg_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # Save weights and print every 10th epoch\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            checkpoint_path = f'checkpoints/resnet_epoch_{epoch+1}.pth'\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'train_loss': avg_loss,\n",
    "                'train_acc': train_acc,\n",
    "                'val_acc': val_acc,\n",
    "                'train_losses': train_losses,\n",
    "                'train_accuracies': train_accuracies,\n",
    "                'val_accuracies': val_accuracies,\n",
    "            }, checkpoint_path)\n",
    "            \n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {avg_loss:.4f} Train Acc: {train_acc:.4f} Val Acc: {val_acc:.4f}\")\n",
    "            print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "        \n",
    "        # Clear memory between epochs\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "        elif device == \"mps\":\n",
    "            torch.mps.empty_cache()\n",
    "        # For CPU, we only use gc.collect() which is called below\n",
    "        gc.collect()\n",
    "    \n",
    "    return train_losses, train_accuracies, val_accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c13e6800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fresh training from epoch 1\n",
      "Epoch [10/200] Loss: 1.7100 Train Acc: 0.5211 Val Acc: 0.4266\n",
      "Checkpoint saved to checkpoints/resnet_epoch_10.pth\n",
      "Epoch [20/200] Loss: 1.2976 Train Acc: 0.6277 Val Acc: 0.4898\n",
      "Checkpoint saved to checkpoints/resnet_epoch_20.pth\n",
      "Epoch [30/200] Loss: 1.1403 Train Acc: 0.6664 Val Acc: 0.4885\n",
      "Checkpoint saved to checkpoints/resnet_epoch_30.pth\n",
      "Epoch [40/200] Loss: 1.0400 Train Acc: 0.6921 Val Acc: 0.4907\n",
      "Checkpoint saved to checkpoints/resnet_epoch_40.pth\n",
      "Epoch [50/200] Loss: 0.9663 Train Acc: 0.7107 Val Acc: 0.5117\n",
      "Checkpoint saved to checkpoints/resnet_epoch_50.pth\n",
      "Epoch [60/200] Loss: 0.9090 Train Acc: 0.7259 Val Acc: 0.5390\n",
      "Checkpoint saved to checkpoints/resnet_epoch_60.pth\n",
      "Epoch [70/200] Loss: 0.8543 Train Acc: 0.7405 Val Acc: 0.5392\n",
      "Checkpoint saved to checkpoints/resnet_epoch_70.pth\n",
      "Epoch [80/200] Loss: 0.7946 Train Acc: 0.7564 Val Acc: 0.5411\n",
      "Checkpoint saved to checkpoints/resnet_epoch_80.pth\n",
      "Epoch [90/200] Loss: 0.7241 Train Acc: 0.7785 Val Acc: 0.5278\n",
      "Checkpoint saved to checkpoints/resnet_epoch_90.pth\n",
      "Epoch [100/200] Loss: 0.6579 Train Acc: 0.7971 Val Acc: 0.5922\n",
      "Checkpoint saved to checkpoints/resnet_epoch_100.pth\n",
      "Epoch [110/200] Loss: 0.5861 Train Acc: 0.8166 Val Acc: 0.5505\n",
      "Checkpoint saved to checkpoints/resnet_epoch_110.pth\n",
      "Epoch [120/200] Loss: 0.4968 Train Acc: 0.8449 Val Acc: 0.6099\n",
      "Checkpoint saved to checkpoints/resnet_epoch_120.pth\n",
      "Epoch [130/200] Loss: 0.4221 Train Acc: 0.8687 Val Acc: 0.6028\n",
      "Checkpoint saved to checkpoints/resnet_epoch_130.pth\n",
      "Epoch [140/200] Loss: 0.3255 Train Acc: 0.8988 Val Acc: 0.6319\n",
      "Checkpoint saved to checkpoints/resnet_epoch_140.pth\n",
      "Epoch [150/200] Loss: 0.2338 Train Acc: 0.9297 Val Acc: 0.6429\n",
      "Checkpoint saved to checkpoints/resnet_epoch_150.pth\n",
      "Epoch [160/200] Loss: 0.1468 Train Acc: 0.9595 Val Acc: 0.6615\n",
      "Checkpoint saved to checkpoints/resnet_epoch_160.pth\n",
      "Epoch [170/200] Loss: 0.0742 Train Acc: 0.9847 Val Acc: 0.6770\n",
      "Checkpoint saved to checkpoints/resnet_epoch_170.pth\n",
      "Epoch [180/200] Loss: 0.0433 Train Acc: 0.9944 Val Acc: 0.6946\n",
      "Checkpoint saved to checkpoints/resnet_epoch_180.pth\n",
      "Epoch [190/200] Loss: 0.0320 Train Acc: 0.9971 Val Acc: 0.7045\n",
      "Checkpoint saved to checkpoints/resnet_epoch_190.pth\n",
      "Epoch [200/200] Loss: 0.0296 Train Acc: 0.9974 Val Acc: 0.7044\n",
      "Checkpoint saved to checkpoints/resnet_epoch_200.pth\n"
     ]
    }
   ],
   "source": [
    "# Usage example: Start training with automatic checkpoint resuming\n",
    "# This will automatically find and load the latest checkpoint if available\n",
    "train_losses, train_accuracies, val_accuracies = train_with_resume(\n",
    "    model=model,\n",
    "    optimizer=optimizer, \n",
    "    scheduler=scheduler,\n",
    "    criterion=criterion,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    num_epochs=num_epochs,\n",
    "    device=device,\n",
    "    resume_from_checkpoint=False  # Set to False to start fresh\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b17d1470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1073cbdf0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20173134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "num_epochs = 300\n",
    "batch_size = 128\n",
    "initial_lr = 0.1\n",
    "momentum = 0.9\n",
    "weight_decay = 5e-4\n",
    "eta_min = 0.0\n",
    "T_max = 280\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=initial_lr,\n",
    "    momentum=momentum,\n",
    "    weight_decay=weight_decay,\n",
    "    nesterov=True\n",
    ")\n",
    "\n",
    "# Scheduler\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=T_max,   # total epochs\n",
    "    eta_min=eta_min  # min LR\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bb75723",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9668daf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fresh training from epoch 1\n",
      "Epoch [10/300] Loss: 1.7600 Train Acc: 0.5098 Val Acc: 0.4057\n",
      "Checkpoint saved to checkpoints/resnet_epoch_10.pth\n",
      "Epoch [20/300] Loss: 1.5222 Train Acc: 0.5685 Val Acc: 0.4571\n",
      "Checkpoint saved to checkpoints/resnet_epoch_20.pth\n",
      "Epoch [30/300] Loss: 1.4403 Train Acc: 0.5900 Val Acc: 0.4880\n",
      "Checkpoint saved to checkpoints/resnet_epoch_30.pth\n",
      "Epoch [40/300] Loss: 1.3907 Train Acc: 0.6031 Val Acc: 0.4802\n",
      "Checkpoint saved to checkpoints/resnet_epoch_40.pth\n",
      "Epoch [50/300] Loss: 1.3491 Train Acc: 0.6114 Val Acc: 0.5057\n",
      "Checkpoint saved to checkpoints/resnet_epoch_50.pth\n",
      "Epoch [60/300] Loss: 1.3098 Train Acc: 0.6227 Val Acc: 0.4951\n",
      "Checkpoint saved to checkpoints/resnet_epoch_60.pth\n",
      "Epoch [70/300] Loss: 1.2753 Train Acc: 0.6317 Val Acc: 0.5043\n",
      "Checkpoint saved to checkpoints/resnet_epoch_70.pth\n",
      "Epoch [80/300] Loss: 1.2481 Train Acc: 0.6396 Val Acc: 0.5123\n",
      "Checkpoint saved to checkpoints/resnet_epoch_80.pth\n",
      "Epoch [90/300] Loss: 1.2154 Train Acc: 0.6491 Val Acc: 0.5376\n",
      "Checkpoint saved to checkpoints/resnet_epoch_90.pth\n",
      "Epoch [100/300] Loss: 1.1770 Train Acc: 0.6565 Val Acc: 0.5328\n",
      "Checkpoint saved to checkpoints/resnet_epoch_100.pth\n",
      "Epoch [110/300] Loss: 1.1451 Train Acc: 0.6665 Val Acc: 0.5175\n",
      "Checkpoint saved to checkpoints/resnet_epoch_110.pth\n",
      "Epoch [120/300] Loss: 1.0969 Train Acc: 0.6796 Val Acc: 0.5506\n",
      "Checkpoint saved to checkpoints/resnet_epoch_120.pth\n",
      "Epoch [130/300] Loss: 1.0504 Train Acc: 0.6895 Val Acc: 0.5502\n",
      "Checkpoint saved to checkpoints/resnet_epoch_130.pth\n",
      "Epoch [140/300] Loss: 1.0008 Train Acc: 0.7016 Val Acc: 0.5680\n",
      "Checkpoint saved to checkpoints/resnet_epoch_140.pth\n",
      "Epoch [150/300] Loss: 0.9446 Train Acc: 0.7181 Val Acc: 0.5977\n",
      "Checkpoint saved to checkpoints/resnet_epoch_150.pth\n",
      "Epoch [160/300] Loss: 0.8899 Train Acc: 0.7331 Val Acc: 0.5710\n",
      "Checkpoint saved to checkpoints/resnet_epoch_160.pth\n",
      "Epoch [170/300] Loss: 0.8234 Train Acc: 0.7512 Val Acc: 0.5775\n",
      "Checkpoint saved to checkpoints/resnet_epoch_170.pth\n",
      "Epoch [180/300] Loss: 0.7674 Train Acc: 0.7659 Val Acc: 0.5963\n",
      "Checkpoint saved to checkpoints/resnet_epoch_180.pth\n",
      "Epoch [190/300] Loss: 0.6867 Train Acc: 0.7904 Val Acc: 0.6188\n",
      "Checkpoint saved to checkpoints/resnet_epoch_190.pth\n",
      "Epoch [200/300] Loss: 0.6031 Train Acc: 0.8156 Val Acc: 0.6191\n",
      "Checkpoint saved to checkpoints/resnet_epoch_200.pth\n",
      "Epoch [210/300] Loss: 0.5244 Train Acc: 0.8377 Val Acc: 0.6227\n",
      "Checkpoint saved to checkpoints/resnet_epoch_210.pth\n",
      "Epoch [220/300] Loss: 0.4260 Train Acc: 0.8676 Val Acc: 0.6471\n",
      "Checkpoint saved to checkpoints/resnet_epoch_220.pth\n",
      "Epoch [230/300] Loss: 0.3268 Train Acc: 0.8992 Val Acc: 0.6439\n",
      "Checkpoint saved to checkpoints/resnet_epoch_230.pth\n",
      "Epoch [240/300] Loss: 0.2291 Train Acc: 0.9320 Val Acc: 0.6673\n",
      "Checkpoint saved to checkpoints/resnet_epoch_240.pth\n",
      "Epoch [250/300] Loss: 0.1450 Train Acc: 0.9638 Val Acc: 0.6759\n",
      "Checkpoint saved to checkpoints/resnet_epoch_250.pth\n",
      "Epoch [260/300] Loss: 0.0968 Train Acc: 0.9799 Val Acc: 0.6930\n",
      "Checkpoint saved to checkpoints/resnet_epoch_260.pth\n",
      "Epoch [270/300] Loss: 0.0725 Train Acc: 0.9879 Val Acc: 0.7003\n",
      "Checkpoint saved to checkpoints/resnet_epoch_270.pth\n",
      "Epoch [280/300] Loss: 0.0664 Train Acc: 0.9903 Val Acc: 0.7002\n",
      "Checkpoint saved to checkpoints/resnet_epoch_280.pth\n",
      "Epoch [290/300] Loss: 0.0675 Train Acc: 0.9908 Val Acc: 0.6995\n",
      "Checkpoint saved to checkpoints/resnet_epoch_290.pth\n",
      "Epoch [300/300] Loss: 0.0715 Train Acc: 0.9889 Val Acc: 0.6920\n",
      "Checkpoint saved to checkpoints/resnet_epoch_300.pth\n"
     ]
    }
   ],
   "source": [
    "# Usage example: Start training with automatic checkpoint resuming\n",
    "# This will automatically find and load the latest checkpoint if available\n",
    "train_losses, train_accuracies, val_accuracies = train_with_resume(\n",
    "    model=model,\n",
    "    optimizer=optimizer, \n",
    "    scheduler=scheduler,\n",
    "    criterion=criterion,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    num_epochs=num_epochs,\n",
    "    device=device,\n",
    "    resume_from_checkpoint=False  # Set to False to start fresh\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64046eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_training_data_from_checkpoints(checkpoint_dir):\n",
    "#     \"\"\"\n",
    "#     Load training data from all checkpoints in a directory\n",
    "#     Returns: epochs, train_losses, val_accuracies\n",
    "#     \"\"\"\n",
    "#     if not os.path.exists(checkpoint_dir):\n",
    "#         print(f\"Directory {checkpoint_dir} does not exist\")\n",
    "#         return [], [], []\n",
    "    \n",
    "#     # Get all checkpoint files\n",
    "#     checkpoint_files = glob.glob(os.path.join(checkpoint_dir, 'resnet_epoch_*.pth'))\n",
    "#     if not checkpoint_files:\n",
    "#         print(f\"No checkpoint files found in {checkpoint_dir}\")\n",
    "#         return [], [], []\n",
    "    \n",
    "#     # Sort by epoch number\n",
    "#     def extract_epoch_number(filename):\n",
    "#         basename = os.path.basename(filename)\n",
    "#         epoch_str = basename.split('_epoch_')[1].split('.pth')[0]\n",
    "#         return int(epoch_str)\n",
    "    \n",
    "#     checkpoint_files.sort(key=extract_epoch_number)\n",
    "    \n",
    "#     epochs = []\n",
    "#     train_losses = []\n",
    "#     val_accuracies = []\n",
    "    \n",
    "#     for checkpoint_file in checkpoint_files:\n",
    "#         try:\n",
    "#             checkpoint = torch.load(checkpoint_file, map_location='cpu')\n",
    "#             epoch = checkpoint['epoch']\n",
    "#             train_loss = checkpoint['train_loss']\n",
    "#             val_acc = checkpoint['val_acc']\n",
    "            \n",
    "#             epochs.append(epoch)\n",
    "#             train_losses.append(train_loss)\n",
    "#             val_accuracies.append(val_acc)\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"Error loading {checkpoint_file}: {e}\")\n",
    "#             continue\n",
    "    \n",
    "#     return epochs, train_losses, val_accuracies\n",
    "\n",
    "# def plot_training_curves(checkpoint_dirs, save_plots=True):\n",
    "#     \"\"\"\n",
    "#     Plot training curves for multiple checkpoint directories\n",
    "#     \"\"\"\n",
    "#     plt.figure(figsize=(15, 5))\n",
    "    \n",
    "#     # Plot 1: Training Loss\n",
    "#     plt.subplot(1, 2, 1)\n",
    "#     for checkpoint_dir in checkpoint_dirs:\n",
    "#         epochs, train_losses, val_accuracies = load_training_data_from_checkpoints(checkpoint_dir)\n",
    "#         if epochs:\n",
    "#             plt.plot(epochs, train_losses, 'o-', label=f'{checkpoint_dir} (Train Loss)', linewidth=2, markersize=4)\n",
    "    \n",
    "#     plt.xlabel('Epoch')\n",
    "#     plt.ylabel('Training Loss')\n",
    "#     plt.title('Training Loss vs Epoch')\n",
    "#     plt.legend()\n",
    "#     plt.grid(True, alpha=0.3)\n",
    "#     plt.yscale('log')  # Log scale for better visualization\n",
    "    \n",
    "#     # Plot 2: Validation Accuracy\n",
    "#     plt.subplot(1, 2, 2)\n",
    "#     for checkpoint_dir in checkpoint_dirs:\n",
    "#         epochs, train_losses, val_accuracies = load_training_data_from_checkpoints(checkpoint_dir)\n",
    "#         if epochs:\n",
    "#             plt.plot(epochs, val_accuracies, 's-', label=f'{checkpoint_dir} (Val Acc)', linewidth=2, markersize=4)\n",
    "    \n",
    "#     plt.xlabel('Epoch')\n",
    "#     plt.ylabel('Validation Accuracy')\n",
    "#     plt.title('Validation Accuracy vs Epoch')\n",
    "#     plt.legend()\n",
    "#     plt.grid(True, alpha=0.3)\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "    \n",
    "#     if save_plots:\n",
    "#         plt.savefig('training_curves_comparison.png', dpi=300, bbox_inches='tight')\n",
    "#         print(\"Plot saved as 'training_curves_comparison.png'\")\n",
    "    \n",
    "#     plt.show()\n",
    "\n",
    "# # Load and plot data from all checkpoint directories\n",
    "# checkpoint_directories = [\n",
    "#     'checkpoints',\n",
    "#     'checkpoints_weight_decay', \n",
    "#     'checkpoints_wrong_scheduler'\n",
    "# ]\n",
    "\n",
    "# print(\"Loading training data from all checkpoint directories...\")\n",
    "# plot_training_curves(checkpoint_directories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f8f99a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly.graph_objects as go\n",
    "# import plotly.express as px\n",
    "# import os\n",
    "# import glob\n",
    "# import torch\n",
    "\n",
    "# def load_training_data_from_checkpoints_plotly(checkpoint_dir):\n",
    "#     \"\"\"\n",
    "#     Load training data from all checkpoints in a directory for Plotly\n",
    "#     Returns: epochs, train_losses, val_accuracies\n",
    "#     \"\"\"\n",
    "#     if not os.path.exists(checkpoint_dir):\n",
    "#         print(f\"Directory {checkpoint_dir} does not exist\")\n",
    "#         return [], [], []\n",
    "    \n",
    "#     # Get all checkpoint files\n",
    "#     checkpoint_files = glob.glob(os.path.join(checkpoint_dir, 'resnet_epoch_*.pth'))\n",
    "#     if not checkpoint_files:\n",
    "#         print(f\"No checkpoint files found in {checkpoint_dir}\")\n",
    "#         return [], [], []\n",
    "    \n",
    "#     # Sort by epoch number\n",
    "#     def extract_epoch_number(filename):\n",
    "#         basename = os.path.basename(filename)\n",
    "#         epoch_str = basename.split('_epoch_')[1].split('.pth')[0]\n",
    "#         return int(epoch_str)\n",
    "    \n",
    "#     checkpoint_files.sort(key=extract_epoch_number)\n",
    "    \n",
    "#     epochs = []\n",
    "#     train_losses = []\n",
    "#     val_accuracies = []\n",
    "    \n",
    "#     for checkpoint_file in checkpoint_files:\n",
    "#         try:\n",
    "#             checkpoint = torch.load(checkpoint_file, map_location='cpu')\n",
    "#             epoch = checkpoint['epoch']\n",
    "#             train_loss = checkpoint['train_loss']\n",
    "#             val_acc = checkpoint['val_acc']\n",
    "            \n",
    "#             epochs.append(epoch)\n",
    "#             train_losses.append(train_loss)\n",
    "#             val_accuracies.append(val_acc)\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"Error loading {checkpoint_file}: {e}\")\n",
    "#             continue\n",
    "    \n",
    "#     return epochs, train_losses, val_accuracies\n",
    "\n",
    "# def create_single_plotly_graph(checkpoint_dirs):\n",
    "#     \"\"\"\n",
    "#     Create a single interactive Plotly plot with both training loss and validation accuracy\n",
    "#     \"\"\"\n",
    "#     fig = go.Figure()\n",
    "    \n",
    "#     # Color palette for different experiments\n",
    "#     colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']\n",
    "    \n",
    "#     for i, checkpoint_dir in enumerate(checkpoint_dirs):\n",
    "#         epochs, train_losses, val_accuracies = load_training_data_from_checkpoints_plotly(checkpoint_dir)\n",
    "        \n",
    "#         if epochs:\n",
    "#             # Clean up directory name for display\n",
    "#             display_name = checkpoint_dir.replace('checkpoints', '').replace('_', ' ').strip()\n",
    "#             if not display_name:\n",
    "#                 display_name = 'Original'\n",
    "#             else:\n",
    "#                 display_name = display_name.title()\n",
    "            \n",
    "#             # Add training loss trace\n",
    "#             fig.add_trace(\n",
    "#                 go.Scatter(\n",
    "#                     x=epochs, \n",
    "#                     y=train_losses,\n",
    "#                     mode='lines+markers',\n",
    "#                     name=f'{display_name} - Train Loss',\n",
    "#                     line=dict(color=colors[i % len(colors)], width=3),\n",
    "#                     marker=dict(size=6, symbol='circle'),\n",
    "#                     yaxis='y',\n",
    "#                     hovertemplate='<b>%{fullData.name}</b><br>' +\n",
    "#                                 'Epoch: %{x}<br>' +\n",
    "#                                 'Loss: %{y:.4f}<br>' +\n",
    "#                                 '<extra></extra>'\n",
    "#                 )\n",
    "#             )\n",
    "            \n",
    "#             # Add validation accuracy trace\n",
    "#             fig.add_trace(\n",
    "#                 go.Scatter(\n",
    "#                     x=epochs, \n",
    "#                     y=val_accuracies,\n",
    "#                     mode='lines+markers',\n",
    "#                     name=f'{display_name} - Val Acc',\n",
    "#                     line=dict(color=colors[i % len(colors)], width=3, dash='dash'),\n",
    "#                     marker=dict(size=6, symbol='square'),\n",
    "#                     yaxis='y2',\n",
    "#                     hovertemplate='<b>%{fullData.name}</b><br>' +\n",
    "#                                 'Epoch: %{x}<br>' +\n",
    "#                                 'Accuracy: %{y:.4f}<br>' +\n",
    "#                                 '<extra></extra>'\n",
    "#                 )\n",
    "#             )\n",
    "    \n",
    "#     # Update layout with dual y-axes\n",
    "#     fig.update_layout(\n",
    "#         title={\n",
    "#             'text': \"Training Progress: Loss and Accuracy Comparison\",\n",
    "#             'x': 0.5,\n",
    "#             'xanchor': 'center',\n",
    "#             'font': {'size': 20}\n",
    "#         },\n",
    "#         xaxis=dict(title=\"Epoch\"),\n",
    "#         yaxis=dict(\n",
    "#             title=\"Training Loss\",\n",
    "#             type=\"log\",\n",
    "#             side=\"left\",\n",
    "#             tickfont=dict(color='#1f77b4')\n",
    "#         ),\n",
    "#         yaxis2=dict(\n",
    "#             title=\"Validation Accuracy\",\n",
    "#             side=\"right\",\n",
    "#             overlaying=\"y\",\n",
    "#             range=[0, 1],\n",
    "#             tickfont=dict(color='#ff7f0e')\n",
    "#         ),\n",
    "#         height=600,\n",
    "#         width=1000,\n",
    "#         showlegend=True,\n",
    "#         template=\"plotly_white\",\n",
    "#         font=dict(size=12),\n",
    "#         hovermode='x unified'\n",
    "#     )\n",
    "    \n",
    "#     # Add grid\n",
    "#     fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='lightgray')\n",
    "#     fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='lightgray')\n",
    "    \n",
    "#     return fig\n",
    "\n",
    "# # Create the single interactive plot\n",
    "# checkpoint_directories = [\n",
    "#     'checkpoints',\n",
    "#     'checkpoints_weight_decay', \n",
    "#     'checkpoints_wrong_scheduler'\n",
    "# ]\n",
    "\n",
    "# print(\"Creating single interactive Plotly plot for all checkpoint directories...\")\n",
    "# fig = create_single_plotly_graph(checkpoint_directories)\n",
    "# fig.show()\n",
    "\n",
    "# # Save as HTML for sharing\n",
    "# fig.write_html(\"training_curves_single.html\")\n",
    "# print(\"Single interactive plot saved as 'training_curves_single.html'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccbc7cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset.cleanup_cache_files\n",
    "ds = load_dataset(\"hirundo-io/Noisy-CIFAR-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7d657d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['png', '__key__', '__url__'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['png', '__key__', '__url__'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "812eb158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['png', '__key__', '__url__'],\n",
       "    num_rows: 50000\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77596761",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(ds['train'], batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88ffd04a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train/hamster/04330'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader\n",
    "train_loader.dataset[0]\n",
    "train_loader.dataset[0]['png']\n",
    "train_loader.dataset[0]['__key__']\n",
    "# train_loader.dataset[0]['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21a2ff08",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain_loader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlabel\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: tuple indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "train_loader.dataset[0]['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0243585a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-1.8957, -1.8957, -1.8957,  ..., -1.8957, -1.8957, -1.8957],\n",
       "          [-1.8957, -1.8957, -1.8957,  ..., -1.8957, -1.8957, -1.8957],\n",
       "          [-1.8957, -1.8957,  0.7724,  ...,  1.8426,  1.8426,  1.8426],\n",
       "          ...,\n",
       "          [-1.8957, -1.8957, -0.5030,  ...,  0.3913,  0.7285,  0.7724],\n",
       "          [-1.8957, -1.8957, -0.7229,  ...,  0.4059,  0.4646,  0.4206],\n",
       "          [-1.8957, -1.8957, -0.7815,  ...,  0.1567,  0.0687,  0.1567]],\n",
       " \n",
       "         [[-1.8975, -1.8975, -1.8975,  ..., -1.8975, -1.8975, -1.8975],\n",
       "          [-1.8975, -1.8975, -1.8975,  ..., -1.8975, -1.8975, -1.8975],\n",
       "          [-1.8975, -1.8975,  1.0686,  ...,  2.0012,  2.0012,  2.0012],\n",
       "          ...,\n",
       "          [-1.8975, -1.8975, -0.5979,  ...,  0.9921,  1.4049,  1.4661],\n",
       "          [-1.8975, -1.8975, -0.7508,  ...,  1.0074,  1.1603,  1.1450],\n",
       "          [-1.8975, -1.8975, -0.7202,  ...,  0.7781,  0.7475,  0.8392]],\n",
       " \n",
       "         [[-1.5965, -1.5965, -1.5965,  ..., -1.5965, -1.5965, -1.5965],\n",
       "          [-1.5965, -1.5965, -1.5965,  ..., -1.5965, -1.5965, -1.5965],\n",
       "          [-1.5965, -1.5965,  0.7755,  ...,  2.0254,  2.0254,  2.0254],\n",
       "          ...,\n",
       "          [-1.5965, -1.5965, -0.9290,  ..., -0.2330, -0.0910, -0.0341],\n",
       "          [-1.5965, -1.5965, -1.2272,  ..., -0.2188, -0.4745, -0.6165],\n",
       "          [-1.5965, -1.5965, -1.3125,  ..., -0.5171, -0.8295, -0.7443]]]),\n",
       " 19)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc6f536",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python hirundo_assignment",
   "language": "python",
   "name": "venv_name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
