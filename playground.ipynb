{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "752130c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # If running in a notebook, install missing dependencies with pip before import\n",
    "# import sys\n",
    "# import subprocess\n",
    "\n",
    "# def install_and_import(package, import_name=None):\n",
    "#     import importlib\n",
    "#     try:\n",
    "#         if import_name is None:\n",
    "#             import_name = package\n",
    "#         importlib.import_module(import_name)\n",
    "#     except ImportError:\n",
    "#         print(f\"Installing {package} ...\")\n",
    "#         subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "#         # Optionally try to import again\n",
    "#         importlib.invalidate_caches()\n",
    "#         importlib.import_module(import_name)\n",
    "\n",
    "# # List of (pip_package, import_name) pairs\n",
    "# packages = [\n",
    "#     (\"torch\", \"torch\"),\n",
    "#     (\"torchvision\", \"torchvision\"),\n",
    "#     (\"datasets\", \"datasets\"),\n",
    "#     (\"numpy\", \"numpy\"),\n",
    "# ]\n",
    "\n",
    "# for pip_name, import_name in packages:\n",
    "#     install_and_import(pip_name, import_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c482f68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1160cfc70>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from ResNet import ResNet\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "import glob\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_built() \\\n",
    "    else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(device)\n",
    "\n",
    "torch.manual_seed(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a1eb229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Manually load a specific checkpoint\n",
    "# Uncomment and modify the path to load a specific checkpoint\n",
    "\n",
    "# checkpoint_path = 'checkpoints/resnet_epoch_50.pth'  # Specify the checkpoint you want to load\n",
    "# start_epoch, train_losses, train_accuracies, val_accuracies = load_checkpoint(\n",
    "#     model, optimizer, scheduler, checkpoint_path\n",
    "# )\n",
    "# print(f\"Loaded checkpoint from epoch {start_epoch}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d3aaff",
   "metadata": {},
   "source": [
    "## __Initial training of ResNet-56 on CIFAR10__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cc0bae",
   "metadata": {},
   "source": [
    "#### NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0228fa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ResNet(num_classes=10,n=9).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75ac007",
   "metadata": {},
   "source": [
    "#### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d714fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define standard data transforms for CIFAR10\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.4914, 0.4822, 0.4465],\n",
    "        std=[0.2023, 0.1994, 0.2010]\n",
    "    ),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.4914, 0.4822, 0.4465],\n",
    "        std=[0.2023, 0.1994, 0.2010]\n",
    "    ),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0427d231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_dataset=torchvision.datasets.CIFAR10(root='./data',train=True,download=True,transform=train_transform)\n",
    "test_dataset=torchvision.datasets.CIFAR10(root='./data',train=False,download=True,transform=test_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89ddfd4",
   "metadata": {},
   "source": [
    "#### Define basic params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc28a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "num_epochs = 270\n",
    "batch_size = 1000\n",
    "initial_lr = 0.1\n",
    "\n",
    "# Learning rate schedule: warmup for 15 epochs, then step down\n",
    "def lr_lambda(epoch):\n",
    "    if epoch < 15:\n",
    "        return (epoch + 1) / 15\n",
    "    elif epoch < 90:\n",
    "        return 1.0\n",
    "    elif epoch < 180:\n",
    "        return 0.1\n",
    "    elif epoch < 240:\n",
    "        return 0.01\n",
    "    else:\n",
    "        return 0.001\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=initial_lr, momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1116fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1952e2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint loading functionality\n",
    "def find_latest_checkpoint(checkpoint_dir='checkpoints'):\n",
    "    \"\"\"Find the latest checkpoint file in the directory\"\"\"\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        return None\n",
    "    \n",
    "    checkpoint_files = glob.glob(os.path.join(checkpoint_dir, 'resnet_epoch_*.pth'))\n",
    "    if not checkpoint_files:\n",
    "        return None\n",
    "    \n",
    "    # Sort by epoch number (extract epoch number from filename)\n",
    "    def extract_epoch_number(filename):\n",
    "        # Extract epoch number from filename like 'resnet_epoch_50.pth'\n",
    "        basename = os.path.basename(filename)\n",
    "        epoch_str = basename.split('_epoch_')[1].split('.pth')[0]\n",
    "        return int(epoch_str)\n",
    "    \n",
    "    latest_checkpoint = max(checkpoint_files, key=extract_epoch_number)\n",
    "    return latest_checkpoint\n",
    "\n",
    "def load_checkpoint(model, optimizer, scheduler, checkpoint_path):\n",
    "    \"\"\"Load checkpoint and return epoch number and metrics\"\"\"\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        print(f\"Checkpoint not found: {checkpoint_path}\")\n",
    "        return 0, [], [], []\n",
    "    \n",
    "    print(f\"Loading checkpoint from: {checkpoint_path}\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    # Load model state\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    # Load optimizer state\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    # Load scheduler state\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    \n",
    "    # Get epoch number\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    \n",
    "    print(f\"Resuming from epoch {start_epoch}\")\n",
    "    print(f\"Previous metrics - Loss: {checkpoint['train_loss']:.4f}, \"\n",
    "          f\"Train Acc: {checkpoint['train_acc']:.4f}, Val Acc: {checkpoint['val_acc']:.4f}\")\n",
    "    \n",
    "    return start_epoch, checkpoint.get('train_losses', []), \\\n",
    "           checkpoint.get('train_accuracies', []), checkpoint.get('val_accuracies', [])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2f77fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced training loop with checkpoint resuming\n",
    "def train_with_resume(model, optimizer, scheduler, criterion, train_loader, test_loader, \n",
    "                     num_epochs, device, resume_from_checkpoint=True):\n",
    "    \"\"\"\n",
    "    Training loop that can resume from the latest checkpoint\n",
    "    \"\"\"\n",
    "    # Initialize metrics lists\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    # Create directory for saving weights\n",
    "    os.makedirs('checkpoints', exist_ok=True)\n",
    "    \n",
    "    # Try to resume from checkpoint if requested\n",
    "    start_epoch = 0\n",
    "    if resume_from_checkpoint:\n",
    "        latest_checkpoint = find_latest_checkpoint()\n",
    "        if latest_checkpoint:\n",
    "            start_epoch, train_losses, train_accuracies, val_accuracies = load_checkpoint(\n",
    "                model, optimizer, scheduler, latest_checkpoint\n",
    "            )\n",
    "            print(f\"Resuming training from epoch {start_epoch + 1}\")\n",
    "        else:\n",
    "            print(\"No checkpoint found, starting from epoch 1\")\n",
    "    else:\n",
    "        print(\"Starting fresh training from epoch 1\")\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(images)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        train_acc = correct / total if total > 0 else 0.\n",
    "\n",
    "        del images, labels, output, loss\n",
    "\n",
    "        model.eval()\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                output = model(images)\n",
    "                _, predicted = torch.max(output, 1)\n",
    "                total_val += labels.size(0)\n",
    "                correct_val += (predicted == labels).sum().item()\n",
    "            val_acc = correct_val / total_val if total_val > 0 else 0.\n",
    "        \n",
    "        del images, labels, output\n",
    "\n",
    "        train_losses.append(avg_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # Save weights and print every 10th epoch\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            checkpoint_path = f'checkpoints/resnet_epoch_{epoch+1}.pth'\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'train_loss': avg_loss,\n",
    "                'train_acc': train_acc,\n",
    "                'val_acc': val_acc,\n",
    "                'train_losses': train_losses,\n",
    "                'train_accuracies': train_accuracies,\n",
    "                'val_accuracies': val_accuracies,\n",
    "            }, checkpoint_path)\n",
    "            \n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {avg_loss:.4f} Train Acc: {train_acc:.4f} Val Acc: {val_acc:.4f}\")\n",
    "            print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "        \n",
    "        # Clear memory between epochs\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "        elif device == \"mps\":\n",
    "            torch.mps.empty_cache()\n",
    "        # For CPU, we only use gc.collect() which is called below\n",
    "        gc.collect()\n",
    "    \n",
    "    return train_losses, train_accuracies, val_accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13e6800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fresh training from epoch 1\n"
     ]
    }
   ],
   "source": [
    "# Usage example: Start training with automatic checkpoint resuming\n",
    "# This will automatically find and load the latest checkpoint if available\n",
    "train_losses, train_accuracies, val_accuracies = train_with_resume(\n",
    "    model=model,\n",
    "    optimizer=optimizer, \n",
    "    scheduler=scheduler,\n",
    "    criterion=criterion,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    num_epochs=num_epochs,\n",
    "    device=device,\n",
    "    resume_from_checkpoint=False  # Set to False to start fresh\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99844edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Enhanced training loop with weight saving and memory clearing\n",
    "# train_losses = []\n",
    "# train_accuracies = []\n",
    "# val_accuracies = []\n",
    "\n",
    "# # Create directory for saving weights\n",
    "# os.makedirs('checkpoints', exist_ok=True)\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     total_loss = 0.0\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "\n",
    "#     for images, labels in train_loader:\n",
    "#         images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         output = model(images)\n",
    "#         loss = criterion(output, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "#         _, predicted = torch.max(output, 1)\n",
    "#         total += labels.size(0)\n",
    "#         correct += (predicted == labels).sum().item()\n",
    "    \n",
    "#     avg_loss = total_loss / len(train_loader)\n",
    "#     train_acc = correct / total if total > 0 else 0.\n",
    "\n",
    "#     model.eval()\n",
    "#     correct_val = 0\n",
    "#     total_val = 0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for images, labels in test_loader:\n",
    "#             images, labels = images.to(device), labels.to(device)\n",
    "#             output = model(images)\n",
    "#             _, predicted = torch.max(output, 1)\n",
    "#             total_val += labels.size(0)\n",
    "#             correct_val += (predicted == labels).sum().item()\n",
    "#         val_acc = correct_val / total_val if total_val > 0 else 0.\n",
    "\n",
    "#     train_losses.append(avg_loss)\n",
    "#     train_accuracies.append(train_acc)\n",
    "#     val_accuracies.append(val_acc)\n",
    "\n",
    "#     scheduler.step()\n",
    "\n",
    "#     # Save weights and print every 10th epoch\n",
    "#     if (epoch + 1) % 10 == 0:\n",
    "#         checkpoint_path = f'checkpoints/resnet_epoch_{epoch+1}.pth'\n",
    "#         torch.save({\n",
    "#             'epoch': epoch + 1,\n",
    "#             'model_state_dict': model.state_dict(),\n",
    "#             'optimizer_state_dict': optimizer.state_dict(),\n",
    "#             'scheduler_state_dict': scheduler.state_dict(),\n",
    "#             'train_loss': avg_loss,\n",
    "#             'train_acc': train_acc,\n",
    "#             'val_acc': val_acc,\n",
    "#         }, checkpoint_path)\n",
    "        \n",
    "#         print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {avg_loss:.4f} Train Acc: {train_acc:.4f} Val Acc: {val_acc:.4f}\")\n",
    "#         print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "    \n",
    "#     # Clear memory between epochs\n",
    "#     if device == \"cuda\":\n",
    "#         torch.cuda.empty_cache()\n",
    "#     elif device == \"mps\":\n",
    "#         torch.mps.empty_cache()\n",
    "#     gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cf9360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_losses = []\n",
    "# train_accuracies = []\n",
    "# val_accuracies = []\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     total_loss = 0.0\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "\n",
    "#     for images, labels in train_loader:\n",
    "#         images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         output = model(images)\n",
    "#         loss = criterion(output, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "#         _, predicted = torch.max(output, 1)\n",
    "#         total += labels.size(0)\n",
    "#         correct += (predicted == labels).sum().item()\n",
    "    \n",
    "#     avg_loss = total_loss / len(train_loader)\n",
    "#     train_acc = correct / total if total > 0 else 0.\n",
    "\n",
    "#     model.eval()\n",
    "#     correct_val = 0\n",
    "#     total_val = 0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for images, labels in test_loader:\n",
    "#             images, labels = images.to(device), labels.to(device)\n",
    "#             output = model(images)\n",
    "#             _, predicted = torch.max(output, 1)\n",
    "#             total_val += labels.size(0)\n",
    "#             correct_val += (predicted == labels).sum().item()\n",
    "#         val_acc = correct_val / total_val if total_val > 0 else 0.\n",
    "\n",
    "#     train_losses.append(avg_loss)\n",
    "#     train_accuracies.append(train_acc)\n",
    "#     val_accuracies.append(val_acc)\n",
    "\n",
    "#     scheduler.step()\n",
    "\n",
    "#     # Print every 10th epoch\n",
    "#     if (epoch + 1) % 10 == 0:\n",
    "#         print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {avg_loss:.4f} Train Acc: {train_acc:.4f} Val Acc: {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f5216c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import Dataset\n",
    "# Dataset.cleanup_cache_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8359a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# ds = load_dataset(\"hirundo-io/Noisy-CIFAR-100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcdaa10",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a65392c6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f871cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5ea3ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python hirundo_assignment",
   "language": "python",
   "name": "venv_name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
